# -*- coding: utf-8 -*-
"""mlp_preprocess.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N0HxagW4U_xTgFlvYLZXnVcH6TSWSpeX
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# Load Iris dataset from CSV file
# Replace 'path_to_your_iris.csv' with the actual path to your CSV file
df = pd.read_csv('/content/drive/MyDrive/Deep learning/Iris.csv')

# Assume the dataset has four features and the target label in the last column

# Step 1: Handle missing data (if any)
# Use SimpleImputer to fill in missing values with the mean of the column
imputer = SimpleImputer(strategy='mean')
df.iloc[:, :-1] = imputer.fit_transform(df.iloc[:, :-1])  # Impute missing values in feature columns

# Step 2: Encode categorical target labels (if necessary)
# If your target column is categorical, convert it to numeric using LabelEncoder
label_encoder = LabelEncoder()
df.iloc[:, -1] = label_encoder.fit_transform(df.iloc[:, -1])

# Step 3: Split the dataset into input features (X) and target output (y)
X = df.iloc[:, :-1].values  # Features (all columns except the last)
y = df.iloc[:, -1].astype('int')   # Target (last column) # Convert target variable to integer type

# Step 4: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Standardize the features (scaling helps MLP converge faster)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 6: Create and train the MLP model with specific parameters
mlp = MLPClassifier(
    hidden_layer_sizes=(64, 32, 16),  # Three hidden layers with 64, 32, and 16 neurons
    activation='relu',                # Activation function: 'relu', can use 'tanh', 'logistic', 'identity'
    solver='adam',                    # Optimization algorithm: 'adam', can use 'lbfgs' or 'sgd'
    learning_rate_init=0.01,          # Fixed learning rate (can tune it based on your data)
    max_iter=1000,                    # Maximum number of iterations to train the model
    random_state=42
)

# Step 7: Train the model
mlp.fit(X_train, y_train)

# Step 8: Make predictions on the test set
y_pred = mlp.predict(X_test)

# Step 9: Calculate and print the accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

from google.colab import drive
drive.mount('/content/drive')

"""scaler = StandardScaler()
This line initializes a StandardScaler object. The StandardScaler standardizes features by removing the mean and scaling to unit variance (z-score normalization).

For each feature in the dataset:

𝑋
scaled
=
𝑋
−
𝜇
𝜎
X
scaled
​
 =
σ
X−μ
​

𝜇
μ is the mean of the feature (calculated from the training data).
𝜎
σ is the standard deviation of the feature (also calculated from the training data).
python
Copy code
X_train = scaler.fit_transform(X_train)
The fit_transform method is called on the training data X_train.
fit: It calculates the mean (
𝜇
μ) and standard deviation (
𝜎
σ) for each feature in X_train.
transform: It then uses those statistics to scale each feature in X_train by subtracting the mean and dividing by the standard deviation.
Output: A transformed version of X_train, where each feature has a mean of 0 and a standard deviation of 1. This ensures that the features are on the same scale, which is beneficial for many machine learning algorithms, especially those involving gradient-based optimization.
python
Copy code
X_test = scaler.transform(X_test)
The transform method is applied to X_test.
This step does not recalculate the mean and standard deviation for X_test. Instead, it uses the statistics (mean and standard deviation) calculated from X_train to scale the test data.
Output: A scaled version of X_test, but using the mean and standard deviation from the training set (X_train). This ensures that the model can make predictions on data that is scaled in the same way as the training data.
Summary of the Outputs:
After these operations, both X_train and X_test will be scaled versions of the original datasets.
X_train will have a mean of 0 and a standard deviation of 1 for each feature.
X_test will also be scaled using the same scaling parameters derived from X_train, but it won’t necessarily have a mean of 0 or standard deviation of 1, because the test data is scaled using the training data’s statistics.
This scaling helps ensure that machine learning models like neural networks, logistic regression, and others that rely on gradient descent converge faster and perform better.
"""