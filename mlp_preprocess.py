# -*- coding: utf-8 -*-
"""mlp_preprocess.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N0HxagW4U_xTgFlvYLZXnVcH6TSWSpeX
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# Load Iris dataset from CSV file
# Replace 'path_to_your_iris.csv' with the actual path to your CSV file
df = pd.read_csv('/content/drive/MyDrive/Deep learning/Iris.csv')

# Assume the dataset has four features and the target label in the last column

# Step 1: Handle missing data (if any)
# Use SimpleImputer to fill in missing values with the mean of the column
imputer = SimpleImputer(strategy='mean')
df.iloc[:, :-1] = imputer.fit_transform(df.iloc[:, :-1])  # Impute missing values in feature columns

# Step 2: Encode categorical target labels (if necessary)
# If your target column is categorical, convert it to numeric using LabelEncoder
label_encoder = LabelEncoder()
df.iloc[:, -1] = label_encoder.fit_transform(df.iloc[:, -1])

# Step 3: Split the dataset into input features (X) and target output (y)
X = df.iloc[:, :-1].values  # Features (all columns except the last)
y = df.iloc[:, -1].astype('int')   # Target (last column) # Convert target variable to integer type

# Step 4: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Standardize the features (scaling helps MLP converge faster)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 6: Create and train the MLP model with specific parameters
mlp = MLPClassifier(
    hidden_layer_sizes=(64, 32, 16),  # Three hidden layers with 64, 32, and 16 neurons
    activation='relu',                # Activation function: 'relu', can use 'tanh', 'logistic', 'identity'
    solver='adam',                    # Optimization algorithm: 'adam', can use 'lbfgs' or 'sgd'
    learning_rate_init=0.01,          # Fixed learning rate (can tune it based on your data)
    max_iter=1000,                    # Maximum number of iterations to train the model
    random_state=42
)

# Step 7: Train the model
mlp.fit(X_train, y_train)

# Step 8: Make predictions on the test set
y_pred = mlp.predict(X_test)

# Step 9: Calculate and print the accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

from google.colab import drive
drive.mount('/content/drive')

"""scaler = StandardScaler()
This line initializes a StandardScaler object. The StandardScaler standardizes features by removing the mean and scaling to unit variance (z-score normalization).

For each feature in the dataset:

ğ‘‹
scaled
=
ğ‘‹
âˆ’
ğœ‡
ğœ
X
scaled
â€‹
 =
Ïƒ
Xâˆ’Î¼
â€‹

ğœ‡
Î¼ is the mean of the feature (calculated from the training data).
ğœ
Ïƒ is the standard deviation of the feature (also calculated from the training data).
python
Copy code
X_train = scaler.fit_transform(X_train)
The fit_transform method is called on the training data X_train.
fit: It calculates the mean (
ğœ‡
Î¼) and standard deviation (
ğœ
Ïƒ) for each feature in X_train.
transform: It then uses those statistics to scale each feature in X_train by subtracting the mean and dividing by the standard deviation.
Output: A transformed version of X_train, where each feature has a mean of 0 and a standard deviation of 1. This ensures that the features are on the same scale, which is beneficial for many machine learning algorithms, especially those involving gradient-based optimization.
python
Copy code
X_test = scaler.transform(X_test)
The transform method is applied to X_test.
This step does not recalculate the mean and standard deviation for X_test. Instead, it uses the statistics (mean and standard deviation) calculated from X_train to scale the test data.
Output: A scaled version of X_test, but using the mean and standard deviation from the training set (X_train). This ensures that the model can make predictions on data that is scaled in the same way as the training data.
Summary of the Outputs:
After these operations, both X_train and X_test will be scaled versions of the original datasets.
X_train will have a mean of 0 and a standard deviation of 1 for each feature.
X_test will also be scaled using the same scaling parameters derived from X_train, but it wonâ€™t necessarily have a mean of 0 or standard deviation of 1, because the test data is scaled using the training dataâ€™s statistics.
This scaling helps ensure that machine learning models like neural networks, logistic regression, and others that rely on gradient descent converge faster and perform better.
"""