{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIH0DtHrJU29",
        "outputId": "6d0cc16e-c24a-45f2-dff3-46508417574a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset from CSV file\n",
        "# Replace 'path_to_your_iris.csv' with the actual path to your CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Deep learning/Iris.csv')\n",
        "\n",
        "# Assume the dataset has four features and the target label in the last column\n",
        "X = df.iloc[:, :-1].values  # Features (all columns except the last)\n",
        "y = df.iloc[:, -1].values   # Target (last column)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (scaling helps MLP converge faster)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create and train the MLP model with specific parameters\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(64, 32, 16),  # Three hidden layers with 64, 32, and 16 neurons\n",
        "    activation='relu',                # Activation function: 'relu', can use 'tanh', 'logistic', 'identity'\n",
        "    solver='adam',                    # Optimization algorithm: 'adam', can use 'lbfgs' or 'sgd'\n",
        "    learning_rate_init=0.01,          # Fixed learning rate (can tune it based on your data)\n",
        "    max_iter=1000,                    # Maximum number of iterations to train the model\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a Python code that handles the Iris dataset from a CSV file and allows you to customize the following aspects of the Multilayer Perceptron (MLP):\n",
        "\n",
        "Learning rate\n",
        "Input and output layers\n",
        "Multiple hidden layers\n",
        "Activation function\n",
        "Optimization technique (solver)"
      ],
      "metadata": {
        "id": "3-ZzOrWOKm-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customizable Parameters:\n",
        "\n",
        "Multiple Hidden Layers:\n",
        "\n",
        "You can define multiple hidden layers by setting hidden_layer_sizes=(64, 32, 16) where 64, 32, and 16 are the number of neurons in each hidden layer. You can adjust the number and size of these layers.\n",
        "\n",
        "Activation Function:\n",
        "\n",
        "Set the activation parameter to choose the activation function:\n",
        "'relu': Rectified Linear Unit (commonly used, default).\n",
        "'tanh': Hyperbolic tangent.\n",
        "'logistic': Sigmoid.\n",
        "'identity': No activation, which means the function will just be a linear model.\n",
        "\n",
        "Optimization Technique (Solver):\n",
        "\n",
        "The solver parameter allows you to choose the optimization technique:\n",
        "'adam': Adaptive Moment Estimation (default).\n",
        "'lbfgs': Limited-memory Broyden–Fletcher–Goldfarb–Shanno (quasi-Newton method).\n",
        "'sgd': Stochastic Gradient Descent (slower but can work better for certain datasets).\n",
        "\n",
        "Learning Rate:\n",
        "\n",
        "learning_rate_init=0.01 sets the initial learning rate. You can adjust this depending on how fast you want the model to converge. Too high can cause the model to skip the optimal solution, while too low can make training slower.\n",
        "\n",
        "Input and Output:\n",
        "\n",
        "Inputs (X) come from the first columns of the CSV, while the target (y) comes from the last column.\n",
        "\n",
        "Maximum Iterations:\n",
        "\n",
        "max_iter=1000 defines the maximum number of iterations the model will perform during training. You can adjust this if your model doesn't converge within the given iterations.\n",
        "\n",
        "Notes:\n",
        "You can tune the size of hidden layers, the learning rate, the activation function, and the optimization method to improve performance based on the dataset.\n",
        "Ensure your CSV file is formatted correctly (features in the first columns and target labels in the last)."
      ],
      "metadata": {
        "id": "4tBHW7e0KruJ"
      }
    }
  ]
}